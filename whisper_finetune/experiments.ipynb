{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/whisper-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3fb5f07d1d464e9b5d1eb766328ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99387c33dda4d6e9cf9e8e4f0fbce92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6ec4e752f54d40a04f5b990483895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb82ab09962e4b0d830f7de5f4637c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfe2d2f82ae4a1b959e0f7f69b116b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4181acd71942e38b48bb1fb275d2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1968300acd3940e28873c05ee3af8a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f664d1810bd45a48ed7164bb3ac26ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f44262b04344fe9b29557a492d0305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3d3f49067b435b9ea9ddcd98e625f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f039af04364bed8a47b3edc389d958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "WhisperForConditionalGeneration(\n",
      "  (model): WhisperModel(\n",
      "    (encoder): WhisperEncoder(\n",
      "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (embed_positions): Embedding(1500, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x WhisperEncoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): WhisperDecoder(\n",
      "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
      "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x WhisperDecoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
      ")\n",
      "\n",
      "Number of parameters: 763,857,920\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "# Load the Whisper medium model and processor directly from Hugging Face\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "# Load the model and processor using Auto classes\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
    "\n",
    "# Print the model architecture\n",
    "print(\"\\nModel Summary:\")\n",
    "print(model)\n",
    "\n",
    "# Print some basic information about the model\n",
    "print(f\"\\nNumber of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WHISPER CONFIG PARAMETERS ===\n",
      "vocab_size: 51865\n",
      "num_mel_bins: 80\n",
      "d_model: 384\n",
      "encoder_layers: 4\n",
      "encoder_attention_heads: 6\n",
      "decoder_layers: 4\n",
      "decoder_attention_heads: 6\n",
      "decoder_ffn_dim: 1536\n",
      "encoder_ffn_dim: 1536\n",
      "dropout: 0.0\n",
      "attention_dropout: 0.0\n",
      "activation_dropout: 0.0\n",
      "activation_function: gelu\n",
      "init_std: 0.02\n",
      "encoder_layerdrop: 0.0\n",
      "decoder_layerdrop: 0.0\n",
      "use_cache: True\n",
      "num_hidden_layers: 4\n",
      "scale_embedding: False\n",
      "max_source_positions: 1500\n",
      "max_target_positions: 448\n",
      "classifier_proj_size: 256\n",
      "use_weighted_layer_sum: False\n",
      "apply_spec_augment: False\n",
      "mask_time_prob: 0.05\n",
      "mask_time_length: 10\n",
      "mask_time_min_masks: 2\n",
      "mask_feature_prob: 0.0\n",
      "mask_feature_length: 10\n",
      "mask_feature_min_masks: 0\n",
      "median_filter_width: 7\n",
      "return_dict: True\n",
      "output_hidden_states: False\n",
      "output_attentions: False\n",
      "torchscript: False\n",
      "torch_dtype: None\n",
      "use_bfloat16: False\n",
      "tf_legacy_loss: False\n",
      "pruned_heads: {}\n",
      "tie_word_embeddings: True\n",
      "chunk_size_feed_forward: 0\n",
      "is_encoder_decoder: True\n",
      "is_decoder: False\n",
      "cross_attention_hidden_size: None\n",
      "add_cross_attention: False\n",
      "tie_encoder_decoder: False\n",
      "max_length: 20\n",
      "min_length: 0\n",
      "do_sample: False\n",
      "early_stopping: False\n",
      "num_beams: 1\n",
      "num_beam_groups: 1\n",
      "diversity_penalty: 0.0\n",
      "temperature: 1.0\n",
      "top_k: 50\n",
      "top_p: 1.0\n",
      "typical_p: 1.0\n",
      "repetition_penalty: 1.0\n",
      "length_penalty: 1.0\n",
      "no_repeat_ngram_size: 0\n",
      "encoder_no_repeat_ngram_size: 0\n",
      "bad_words_ids: None\n",
      "num_return_sequences: 1\n",
      "output_scores: False\n",
      "return_dict_in_generate: False\n",
      "forced_bos_token_id: None\n",
      "forced_eos_token_id: None\n",
      "remove_invalid_values: False\n",
      "exponential_decay_length_penalty: None\n",
      "suppress_tokens: None\n",
      "begin_suppress_tokens: [220, 50256]\n",
      "architectures: None\n",
      "finetuning_task: None\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "tokenizer_class: None\n",
      "prefix: None\n",
      "bos_token_id: 50256\n",
      "pad_token_id: 50256\n",
      "eos_token_id: 50256\n",
      "sep_token_id: None\n",
      "decoder_start_token_id: 50257\n",
      "task_specific_params: None\n",
      "problem_type: None\n",
      "_name_or_path: \n",
      "_attn_implementation_autoset: False\n",
      "transformers_version: 4.51.3\n",
      "model_type: whisper\n",
      "\n",
      "=== CONFIG METHODS ===\n",
      "- dict_torch_dtype_to_str(d: dict[str, typing.Any]) -> None\n",
      "- from_dict(config_dict: dict[str, typing.Any], **kwargs) -> 'PretrainedConfig'\n",
      "- from_json_file(json_file: Union[str, os.PathLike]) -> 'PretrainedConfig'\n",
      "- from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> 'PretrainedConfig'\n",
      "- get_config_dict(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]\n",
      "- get_text_config(decoder=False) -> 'PretrainedConfig'\n",
      "- push_to_hub(repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str\n",
      "- register_for_auto_class(auto_class='AutoConfig')\n",
      "- save_pretrained(save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      "- to_dict() -> dict[str, typing.Any]\n",
      "- to_diff_dict() -> dict[str, typing.Any]\n",
      "- to_json_file(json_file_path: Union[str, os.PathLike], use_diff: bool = True)\n",
      "- to_json_string(use_diff: bool = True) -> str\n",
      "- update(config_dict: dict[str, typing.Any])\n",
      "- update_from_string(update_str: str)\n",
      "\n",
      "=== KEY CONFIG PARAMETERS EXPLAINED ===\n",
      "- vocab_size: 51865\n",
      "  Size of the tokenizer vocabulary\n",
      "- d_model: 384\n",
      "  Dimension of the model's hidden representations\n",
      "- encoder_layers: 4\n",
      "  Number of encoder layers in the Transformer\n",
      "- encoder_attention_heads: 6\n",
      "  Number of attention heads in encoder's multi-head attention\n",
      "- decoder_layers: 4\n",
      "  Number of decoder layers in the Transformer\n",
      "- decoder_attention_heads: 6\n",
      "  Number of attention heads in decoder's multi-head attention\n",
      "- max_source_positions: 1500\n",
      "  Maximum sequence length for encoder input\n",
      "- max_target_positions: 448\n",
      "  Maximum sequence length for decoder output\n",
      "- activation_function: gelu\n",
      "  Activation function used in the feed-forward networks\n",
      "- num_mel_bins: 80\n",
      "  Number of mel features for audio processing\n",
      "- dropout: 0.0\n",
      "  Dropout probability throughout the model\n",
      "- attention_dropout: 0.0\n",
      "  Dropout probability for attention weights\n",
      "- activation_dropout: 0.0\n",
      "  Dropout probability after activation in FFN\n",
      "\n",
      "=== MODEL SIZE CONFIGURATIONS ===\n",
      "- Tiny model:\n",
      "  d_model=384, encoder_layers=4, decoder_layers=4\n",
      "  Approximate parameters: ~4.7M\n",
      "- Base model:\n",
      "  d_model=512, encoder_layers=6, decoder_layers=6\n",
      "  Approximate parameters: ~12.6M\n",
      "- Small model:\n",
      "  d_model=768, encoder_layers=12, decoder_layers=12\n",
      "  Approximate parameters: ~56.6M\n",
      "- Medium model:\n",
      "  d_model=1024, encoder_layers=24, decoder_layers=24\n",
      "  Approximate parameters: ~201.3M\n",
      "- Large model:\n",
      "  d_model=1280, encoder_layers=32, decoder_layers=32\n",
      "  Approximate parameters: ~419.4M\n",
      "\n",
      "=== HOW CONFIG INFLUENCES MODEL ARCHITECTURE ===\n",
      "The config parameters determine:\n",
      "1. Size of embedding matrices (vocab_size × d_model)\n",
      "2. Number of encoder and decoder layers\n",
      "3. Size of feed-forward networks (d_model × 4 × d_model)\n",
      "4. Number of attention heads in multi-head attention\n",
      "5. Audio preprocessing parameters (sampling_rate, num_mel_bins)\n",
      "\n",
      "=== CUSTOMIZING CONFIG EXAMPLE ===\n",
      "\n",
      "# Create a customized Whisper config\n",
      "custom_config = WhisperConfig(\n",
      "    vocab_size=51865,              # Keep original vocabulary\n",
      "    d_model=512,                   # Smaller dimension (like base model)\n",
      "    encoder_layers=8,              # Custom number of layers\n",
      "    decoder_layers=8,              # Custom number of layers\n",
      "    encoder_attention_heads=8,     # Number of attention heads\n",
      "    decoder_attention_heads=8,     # Number of attention heads\n",
      "    dropout=0.2,                   # Higher dropout for regularization\n",
      "    attention_dropout=0.1,         # Attention-specific dropout\n",
      "    activation_function=\"gelu\",    # Activation function\n",
      "    max_source_positions=1500,     # Max length for audio input\n",
      "    max_target_positions=448       # Max length for text output\n",
      ")\n",
      "\n",
      "# Initialize a model with custom config\n",
      "from transformers import WhisperForConditionalGeneration\n",
      "custom_model = WhisperForConditionalGeneration(custom_config)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring the WhisperConfig in detail\n",
    "from transformers import WhisperConfig\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "# Create a sample config and get all attributes\n",
    "config = WhisperConfig()\n",
    "\n",
    "# Print all configuration parameters with their default values\n",
    "print(\"=== WHISPER CONFIG PARAMETERS ===\")\n",
    "config_dict = config.to_dict()\n",
    "for key, value in config_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Show what methods are available on the config\n",
    "print(\"\\n=== CONFIG METHODS ===\")\n",
    "config_methods = [method for method in dir(config) if not method.startswith('_') and callable(getattr(config, method))]\n",
    "for method in config_methods:\n",
    "    print(f\"- {method}{inspect.signature(getattr(config, method))}\")\n",
    "\n",
    "# Explain the meaning of key config parameters\n",
    "print(\"\\n=== KEY CONFIG PARAMETERS EXPLAINED ===\")\n",
    "key_params = {\n",
    "    \"vocab_size\": \"Size of the tokenizer vocabulary\",\n",
    "    \"d_model\": \"Dimension of the model's hidden representations\",\n",
    "    \"encoder_layers\": \"Number of encoder layers in the Transformer\",\n",
    "    \"encoder_attention_heads\": \"Number of attention heads in encoder's multi-head attention\",\n",
    "    \"decoder_layers\": \"Number of decoder layers in the Transformer\",\n",
    "    \"decoder_attention_heads\": \"Number of attention heads in decoder's multi-head attention\",\n",
    "    \"max_source_positions\": \"Maximum sequence length for encoder input\",\n",
    "    \"max_target_positions\": \"Maximum sequence length for decoder output\",\n",
    "    \"activation_function\": \"Activation function used in the feed-forward networks\",\n",
    "    \"num_mel_bins\": \"Number of mel features for audio processing\",\n",
    "    \"dropout\": \"Dropout probability throughout the model\",\n",
    "    \"attention_dropout\": \"Dropout probability for attention weights\",\n",
    "    \"activation_dropout\": \"Dropout probability after activation in FFN\"\n",
    "}\n",
    "\n",
    "for param, explanation in key_params.items():\n",
    "    if param in config_dict:\n",
    "        print(f\"- {param}: {config_dict[param]}\")\n",
    "        print(f\"  {explanation}\")\n",
    "\n",
    "# Show the differences between model sizes\n",
    "print(\"\\n=== MODEL SIZE CONFIGURATIONS ===\")\n",
    "model_sizes = {\n",
    "    \"tiny\": {\"d_model\": 384, \"encoder_layers\": 4, \"decoder_layers\": 4},\n",
    "    \"base\": {\"d_model\": 512, \"encoder_layers\": 6, \"decoder_layers\": 6},\n",
    "    \"small\": {\"d_model\": 768, \"encoder_layers\": 12, \"decoder_layers\": 12},\n",
    "    \"medium\": {\"d_model\": 1024, \"encoder_layers\": 24, \"decoder_layers\": 24},\n",
    "    \"large\": {\"d_model\": 1280, \"encoder_layers\": 32, \"decoder_layers\": 32},\n",
    "}\n",
    "\n",
    "for size, params in model_sizes.items():\n",
    "    approx_params = (params[\"d_model\"] * params[\"d_model\"] * \n",
    "                    (params[\"encoder_layers\"] + params[\"decoder_layers\"]) * 4) / 1000000\n",
    "    print(f\"- {size.capitalize()} model:\")\n",
    "    print(f\"  d_model={params['d_model']}, encoder_layers={params['encoder_layers']}, \" \n",
    "          f\"decoder_layers={params['decoder_layers']}\")\n",
    "    print(f\"  Approximate parameters: ~{approx_params:.1f}M\")\n",
    "\n",
    "# Examine how config affects model initialization\n",
    "print(\"\\n=== HOW CONFIG INFLUENCES MODEL ARCHITECTURE ===\")\n",
    "print(\"The config parameters determine:\")\n",
    "print(\"1. Size of embedding matrices (vocab_size × d_model)\")\n",
    "print(\"2. Number of encoder and decoder layers\")\n",
    "print(\"3. Size of feed-forward networks (d_model × 4 × d_model)\")\n",
    "print(\"4. Number of attention heads in multi-head attention\")\n",
    "print(\"5. Audio preprocessing parameters (sampling_rate, num_mel_bins)\")\n",
    "\n",
    "# Show how to customize a config\n",
    "print(\"\\n=== CUSTOMIZING CONFIG EXAMPLE ===\")\n",
    "print(\"\"\"\n",
    "# Create a customized Whisper config\n",
    "custom_config = WhisperConfig(\n",
    "    vocab_size=51865,              # Keep original vocabulary\n",
    "    d_model=512,                   # Smaller dimension (like base model)\n",
    "    encoder_layers=8,              # Custom number of layers\n",
    "    decoder_layers=8,              # Custom number of layers\n",
    "    encoder_attention_heads=8,     # Number of attention heads\n",
    "    decoder_attention_heads=8,     # Number of attention heads\n",
    "    dropout=0.2,                   # Higher dropout for regularization\n",
    "    attention_dropout=0.1,         # Attention-specific dropout\n",
    "    activation_function=\"gelu\",    # Activation function\n",
    "    max_source_positions=1500,     # Max length for audio input\n",
    "    max_target_positions=448       # Max length for text output\n",
    ")\n",
    "\n",
    "# Initialize a model with custom config\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "custom_model = WhisperForConditionalGeneration(custom_config)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSOR DETAILS ===\n",
      "Components: ['chat_template', 'feature_extractor', 'tokenizer', 'current_processor', '_in_target_context_manager']\n",
      "\n",
      "Feature Extractor Properties:\n",
      "- chunk_length: 30\n",
      "- dither: 0.0\n",
      "- feature_extractor_type: WhisperFeatureExtractor\n",
      "- feature_size: 80\n",
      "- hop_length: 160\n",
      "- mel_filters: [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.02486259 0.00199082 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.02287177 0.00398164 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.00089752]\n",
      " [0.         0.         0.         ... 0.         0.         0.00044876]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "- model_input_names: ['input_features']\n",
      "- n_fft: 400\n",
      "- n_samples: 480000\n",
      "- nb_max_frames: 3000\n",
      "\n",
      "How audio is processed:\n",
      "Sampling rate: 16000 Hz\n",
      "Feature size: 80\n",
      "Hop length: 160\n",
      "Chunk length: 30 seconds\n",
      "\n",
      "Tokenizer Details:\n",
      "Vocabulary size: 50258\n",
      "Model max length: 1024\n",
      "Special tokens: ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']\n",
      "\n",
      "Sample tokens from vocabulary:\n",
      "- '!': 0\n",
      "- '\"': 1\n",
      "- '#': 2\n",
      "- '$': 3\n",
      "- '%': 4\n",
      "- '&': 5\n",
      "- ''': 6\n",
      "- '(': 7\n",
      "- ')': 8\n",
      "- '*': 9\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "\n",
      "Key structural difference:\n",
      "\n",
      "Generation methods available:\n",
      "- can_generate\n",
      "- generate\n",
      "- generate_with_fallback\n",
      "\n",
      "=== PRACTICAL CAPABILITIES ===\n",
      "Special prompt capabilities for different languages and tasks:\n",
      "\n",
      "Available languages in processor:\n",
      "['<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>']\n",
      "\n",
      "Prompt generation for different tasks:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjapanese\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     67\u001b[0m     prompt_ids \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mget_decoder_prompt_ids(language\u001b[38;5;241m=\u001b[39mlanguage, task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[0;32m---> 68\u001b[0m     prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx-audio-models/lib/python3.9/site-packages/transformers/tokenization_utils.py:1065\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1063\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m-> 1065\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "# Examining Whisper components in detail\n",
    "from transformers import AutoModel, WhisperModel\n",
    "\n",
    "# Load the base model for comparison - use WhisperModel instead of AutoModel\n",
    "try:\n",
    "    base_model = WhisperModel.from_pretrained(\"openai/whisper-medium\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load base model: {e}\")\n",
    "    print(\"Continuing with processor exploration only...\")\n",
    "\n",
    "# Examine the processor in detail\n",
    "print(\"=== PROCESSOR DETAILS ===\")\n",
    "print(f\"Components: {list(processor.__dict__.keys())}\")\n",
    "\n",
    "# Feature extractor examination\n",
    "print(\"\\nFeature Extractor Properties:\")\n",
    "feature_extractor_props = [attr for attr in dir(processor.feature_extractor) \n",
    "                          if not attr.startswith('_') and not callable(getattr(processor.feature_extractor, attr))]\n",
    "for prop in feature_extractor_props[:10]:  # Show first 10 properties\n",
    "    value = getattr(processor.feature_extractor, prop)\n",
    "    print(f\"- {prop}: {value}\")\n",
    "\n",
    "print(\"\\nHow audio is processed:\")\n",
    "print(f\"Sampling rate: {processor.feature_extractor.sampling_rate} Hz\")\n",
    "print(f\"Feature size: {processor.feature_extractor.feature_size}\")\n",
    "print(f\"Hop length: {processor.feature_extractor.hop_length}\")\n",
    "print(f\"Chunk length: {processor.feature_extractor.chunk_length} seconds\")\n",
    "\n",
    "# Tokenizer examination\n",
    "print(\"\\nTokenizer Details:\")\n",
    "print(f\"Vocabulary size: {processor.tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {processor.tokenizer.model_max_length}\")\n",
    "print(f\"Special tokens: {processor.tokenizer.all_special_tokens}\")\n",
    "\n",
    "# Sample of vocabulary\n",
    "print(\"\\nSample tokens from vocabulary:\")\n",
    "sample_tokens = list(processor.tokenizer.get_vocab().items())[:10]\n",
    "for token, id in sample_tokens:\n",
    "    print(f\"- '{token}': {id}\")\n",
    "\n",
    "# Compare model architectures - skip if base_model failed to load\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "\n",
    "# Check for the language modeling head (key difference)\n",
    "print(\"\\nKey structural difference:\")\n",
    "for name, module in model.named_children():\n",
    "    if \"lm_head\" in name:\n",
    "        print(f\"- Found in WhisperForConditionalGeneration: {name} - {type(module)}\")\n",
    "        if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "            print(f\"  Shape: Input {module.in_features} → Output {module.out_features}\")\n",
    "        else:\n",
    "            print(f\"  Module properties: {dir(module)[:10]}\")\n",
    "\n",
    "# Show the generation methods\n",
    "print(\"\\nGeneration methods available:\")\n",
    "generation_methods = [method for method in dir(model) \n",
    "                     if \"generate\" in method and not method.startswith(\"_\")]\n",
    "for method in generation_methods:\n",
    "    print(f\"- {method}\")\n",
    "\n",
    "# Practical demonstration of prompt handling\n",
    "print(\"\\n=== PRACTICAL CAPABILITIES ===\")\n",
    "print(\"Special prompt capabilities for different languages and tasks:\")\n",
    "\n",
    "# Show available languages and tasks\n",
    "print(\"\\nAvailable languages in processor:\")\n",
    "if hasattr(processor, \"tokenizer\") and hasattr(processor.tokenizer, \"additional_special_tokens\"):\n",
    "    language_tokens = [token for token in processor.tokenizer.additional_special_tokens \n",
    "                      if token.startswith(\"<|\") and token.endswith(\"|>\") and len(token) < 10]\n",
    "    print(language_tokens[:10])  # Show first 10 language tokens\n",
    "\n",
    "# Fix the prompt generation code\n",
    "print(\"\\nPrompt generation for different tasks:\")\n",
    "for task in [\"transcribe\", \"translate\"]:\n",
    "    for language in [\"english\", \"french\", \"japanese\"]:\n",
    "        try:\n",
    "            # Handle potential errors in get_decoder_prompt_ids\n",
    "            prompt_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "            if isinstance(prompt_ids, tuple):\n",
    "                # If it returns a tuple, take the first element\n",
    "                prompt_ids = prompt_ids[0] if prompt_ids else []\n",
    "            prompt_tokens = processor.tokenizer.convert_ids_to_tokens(prompt_ids)\n",
    "            print(f\"- {language.capitalize()} {task}: {prompt_tokens}\")\n",
    "        except Exception as e:\n",
    "            print(f\"- Error with {language} {task}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ylacombe/expresso dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0300fd085ba04fc3a234e37b26d9f39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00012.parquet:  11%|#1        | 73.4M/655M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521b733e686448f6a481313e399bc354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00012.parquet:   0%|          | 0.00/505M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97eeda8639a4f75862a09db178f22cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00012.parquet:   0%|          | 0.00/466M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f494f191586248fd8c055a354123a34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00012.parquet:   0%|          | 0.00/522M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450dc7e6be3c4fd4b5b107a1cd4645e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00005-of-00012.parquet:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0614ff288f344f098832e5255d3d1d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00006-of-00012.parquet:   0%|          | 0.00/406M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8753ae61731145d5a7091bea3fd6ac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00007-of-00012.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Expresso dataset from Hugging Face\n",
    "print(\"Loading ylacombe/expresso dataset...\")\n",
    "dataset = load_dataset(\"ylacombe/expresso\")\n",
    "\n",
    "# Print basic dataset information\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Convert to pandas DataFrame for easier analysis\n",
    "print(\"\\nConverting to DataFrame for analysis...\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Check if 'style' column exists\n",
    "if 'style' not in df.columns:\n",
    "    print(\"'style' column not found. Available columns:\")\n",
    "    print(df.columns.tolist())\n",
    "else:\n",
    "    # Basic information about the style column\n",
    "    print(\"\\n=== STYLE COLUMN ANALYSIS ===\")\n",
    "    print(f\"Number of samples: {len(df)}\")\n",
    "    print(f\"Number of unique styles: {df['style'].nunique()}\")\n",
    "    print(\"\\nSample of style values:\")\n",
    "    print(df['style'].sample(5).tolist())\n",
    "    \n",
    "    # Count samples per style class\n",
    "    style_counts = df['style'].value_counts().reset_index()\n",
    "    style_counts.columns = ['Style', 'Count']\n",
    "    \n",
    "    # Display counts in a table\n",
    "    print(\"\\nSamples per style class:\")\n",
    "    print(style_counts.to_string(index=False))\n",
    "    \n",
    "    # Calculate percentage of total for each style\n",
    "    total_samples = len(df)\n",
    "    style_counts['Percentage'] = (style_counts['Count'] / total_samples * 100).round(2)\n",
    "    \n",
    "    # Sort by count for better visualization\n",
    "    style_counts = style_counts.sort_values('Count', ascending=False)\n",
    "    \n",
    "    # Create bar chart of class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(style_counts['Style'], style_counts['Count'])\n",
    "    plt.title('Number of Samples per Style Class')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create pie chart showing distribution\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.pie(style_counts['Count'], labels=style_counts['Style'], autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Style Classes')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    print(\"\\n=== CLASS IMBALANCE ANALYSIS ===\")\n",
    "    max_count = style_counts['Count'].max()\n",
    "    min_count = style_counts['Count'].min()\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    print(f\"Largest class: {style_counts.iloc[0]['Style']} with {max_count} samples\")\n",
    "    print(f\"Smallest class: {style_counts.iloc[-1]['Style']} with {min_count} samples\")\n",
    "    print(f\"Imbalance ratio (largest/smallest): {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(\"\\n=== ADDITIONAL STATISTICS ===\")\n",
    "    print(f\"Mean samples per class: {style_counts['Count'].mean():.2f}\")\n",
    "    print(f\"Median samples per class: {style_counts['Count'].median():.2f}\")\n",
    "    print(f\"Standard deviation: {style_counts['Count'].std():.2f}\")\n",
    "    \n",
    "    # Check if there are any missing values in the style column\n",
    "    missing_styles = df['style'].isna().sum()\n",
    "    print(f\"\\nMissing style values: {missing_styles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-audio-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
