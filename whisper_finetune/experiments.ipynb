{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/whisper-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3fb5f07d1d464e9b5d1eb766328ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99387c33dda4d6e9cf9e8e4f0fbce92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6ec4e752f54d40a04f5b990483895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb82ab09962e4b0d830f7de5f4637c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfe2d2f82ae4a1b959e0f7f69b116b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4181acd71942e38b48bb1fb275d2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1968300acd3940e28873c05ee3af8a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f664d1810bd45a48ed7164bb3ac26ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f44262b04344fe9b29557a492d0305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3d3f49067b435b9ea9ddcd98e625f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f039af04364bed8a47b3edc389d958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "WhisperForConditionalGeneration(\n",
      "  (model): WhisperModel(\n",
      "    (encoder): WhisperEncoder(\n",
      "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (embed_positions): Embedding(1500, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x WhisperEncoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): WhisperDecoder(\n",
      "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
      "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x WhisperDecoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
      ")\n",
      "\n",
      "Number of parameters: 763,857,920\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "# Load the Whisper medium model and processor directly from Hugging Face\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "# Load the model and processor using Auto classes\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
    "\n",
    "# Print the model architecture\n",
    "print(\"\\nModel Summary:\")\n",
    "print(model)\n",
    "\n",
    "# Print some basic information about the model\n",
    "print(f\"\\nNumber of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WHISPER CONFIG PARAMETERS ===\n",
      "vocab_size: 51865\n",
      "num_mel_bins: 80\n",
      "d_model: 384\n",
      "encoder_layers: 4\n",
      "encoder_attention_heads: 6\n",
      "decoder_layers: 4\n",
      "decoder_attention_heads: 6\n",
      "decoder_ffn_dim: 1536\n",
      "encoder_ffn_dim: 1536\n",
      "dropout: 0.0\n",
      "attention_dropout: 0.0\n",
      "activation_dropout: 0.0\n",
      "activation_function: gelu\n",
      "init_std: 0.02\n",
      "encoder_layerdrop: 0.0\n",
      "decoder_layerdrop: 0.0\n",
      "use_cache: True\n",
      "num_hidden_layers: 4\n",
      "scale_embedding: False\n",
      "max_source_positions: 1500\n",
      "max_target_positions: 448\n",
      "classifier_proj_size: 256\n",
      "use_weighted_layer_sum: False\n",
      "apply_spec_augment: False\n",
      "mask_time_prob: 0.05\n",
      "mask_time_length: 10\n",
      "mask_time_min_masks: 2\n",
      "mask_feature_prob: 0.0\n",
      "mask_feature_length: 10\n",
      "mask_feature_min_masks: 0\n",
      "median_filter_width: 7\n",
      "return_dict: True\n",
      "output_hidden_states: False\n",
      "output_attentions: False\n",
      "torchscript: False\n",
      "torch_dtype: None\n",
      "use_bfloat16: False\n",
      "tf_legacy_loss: False\n",
      "pruned_heads: {}\n",
      "tie_word_embeddings: True\n",
      "chunk_size_feed_forward: 0\n",
      "is_encoder_decoder: True\n",
      "is_decoder: False\n",
      "cross_attention_hidden_size: None\n",
      "add_cross_attention: False\n",
      "tie_encoder_decoder: False\n",
      "max_length: 20\n",
      "min_length: 0\n",
      "do_sample: False\n",
      "early_stopping: False\n",
      "num_beams: 1\n",
      "num_beam_groups: 1\n",
      "diversity_penalty: 0.0\n",
      "temperature: 1.0\n",
      "top_k: 50\n",
      "top_p: 1.0\n",
      "typical_p: 1.0\n",
      "repetition_penalty: 1.0\n",
      "length_penalty: 1.0\n",
      "no_repeat_ngram_size: 0\n",
      "encoder_no_repeat_ngram_size: 0\n",
      "bad_words_ids: None\n",
      "num_return_sequences: 1\n",
      "output_scores: False\n",
      "return_dict_in_generate: False\n",
      "forced_bos_token_id: None\n",
      "forced_eos_token_id: None\n",
      "remove_invalid_values: False\n",
      "exponential_decay_length_penalty: None\n",
      "suppress_tokens: None\n",
      "begin_suppress_tokens: [220, 50256]\n",
      "architectures: None\n",
      "finetuning_task: None\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "tokenizer_class: None\n",
      "prefix: None\n",
      "bos_token_id: 50256\n",
      "pad_token_id: 50256\n",
      "eos_token_id: 50256\n",
      "sep_token_id: None\n",
      "decoder_start_token_id: 50257\n",
      "task_specific_params: None\n",
      "problem_type: None\n",
      "_name_or_path: \n",
      "_attn_implementation_autoset: False\n",
      "transformers_version: 4.51.3\n",
      "model_type: whisper\n",
      "\n",
      "=== CONFIG METHODS ===\n",
      "- dict_torch_dtype_to_str(d: dict[str, typing.Any]) -> None\n",
      "- from_dict(config_dict: dict[str, typing.Any], **kwargs) -> 'PretrainedConfig'\n",
      "- from_json_file(json_file: Union[str, os.PathLike]) -> 'PretrainedConfig'\n",
      "- from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> 'PretrainedConfig'\n",
      "- get_config_dict(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]\n",
      "- get_text_config(decoder=False) -> 'PretrainedConfig'\n",
      "- push_to_hub(repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str\n",
      "- register_for_auto_class(auto_class='AutoConfig')\n",
      "- save_pretrained(save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      "- to_dict() -> dict[str, typing.Any]\n",
      "- to_diff_dict() -> dict[str, typing.Any]\n",
      "- to_json_file(json_file_path: Union[str, os.PathLike], use_diff: bool = True)\n",
      "- to_json_string(use_diff: bool = True) -> str\n",
      "- update(config_dict: dict[str, typing.Any])\n",
      "- update_from_string(update_str: str)\n",
      "\n",
      "=== KEY CONFIG PARAMETERS EXPLAINED ===\n",
      "- vocab_size: 51865\n",
      "  Size of the tokenizer vocabulary\n",
      "- d_model: 384\n",
      "  Dimension of the model's hidden representations\n",
      "- encoder_layers: 4\n",
      "  Number of encoder layers in the Transformer\n",
      "- encoder_attention_heads: 6\n",
      "  Number of attention heads in encoder's multi-head attention\n",
      "- decoder_layers: 4\n",
      "  Number of decoder layers in the Transformer\n",
      "- decoder_attention_heads: 6\n",
      "  Number of attention heads in decoder's multi-head attention\n",
      "- max_source_positions: 1500\n",
      "  Maximum sequence length for encoder input\n",
      "- max_target_positions: 448\n",
      "  Maximum sequence length for decoder output\n",
      "- activation_function: gelu\n",
      "  Activation function used in the feed-forward networks\n",
      "- num_mel_bins: 80\n",
      "  Number of mel features for audio processing\n",
      "- dropout: 0.0\n",
      "  Dropout probability throughout the model\n",
      "- attention_dropout: 0.0\n",
      "  Dropout probability for attention weights\n",
      "- activation_dropout: 0.0\n",
      "  Dropout probability after activation in FFN\n",
      "\n",
      "=== MODEL SIZE CONFIGURATIONS ===\n",
      "- Tiny model:\n",
      "  d_model=384, encoder_layers=4, decoder_layers=4\n",
      "  Approximate parameters: ~4.7M\n",
      "- Base model:\n",
      "  d_model=512, encoder_layers=6, decoder_layers=6\n",
      "  Approximate parameters: ~12.6M\n",
      "- Small model:\n",
      "  d_model=768, encoder_layers=12, decoder_layers=12\n",
      "  Approximate parameters: ~56.6M\n",
      "- Medium model:\n",
      "  d_model=1024, encoder_layers=24, decoder_layers=24\n",
      "  Approximate parameters: ~201.3M\n",
      "- Large model:\n",
      "  d_model=1280, encoder_layers=32, decoder_layers=32\n",
      "  Approximate parameters: ~419.4M\n",
      "\n",
      "=== HOW CONFIG INFLUENCES MODEL ARCHITECTURE ===\n",
      "The config parameters determine:\n",
      "1. Size of embedding matrices (vocab_size × d_model)\n",
      "2. Number of encoder and decoder layers\n",
      "3. Size of feed-forward networks (d_model × 4 × d_model)\n",
      "4. Number of attention heads in multi-head attention\n",
      "5. Audio preprocessing parameters (sampling_rate, num_mel_bins)\n",
      "\n",
      "=== CUSTOMIZING CONFIG EXAMPLE ===\n",
      "\n",
      "# Create a customized Whisper config\n",
      "custom_config = WhisperConfig(\n",
      "    vocab_size=51865,              # Keep original vocabulary\n",
      "    d_model=512,                   # Smaller dimension (like base model)\n",
      "    encoder_layers=8,              # Custom number of layers\n",
      "    decoder_layers=8,              # Custom number of layers\n",
      "    encoder_attention_heads=8,     # Number of attention heads\n",
      "    decoder_attention_heads=8,     # Number of attention heads\n",
      "    dropout=0.2,                   # Higher dropout for regularization\n",
      "    attention_dropout=0.1,         # Attention-specific dropout\n",
      "    activation_function=\"gelu\",    # Activation function\n",
      "    max_source_positions=1500,     # Max length for audio input\n",
      "    max_target_positions=448       # Max length for text output\n",
      ")\n",
      "\n",
      "# Initialize a model with custom config\n",
      "from transformers import WhisperForConditionalGeneration\n",
      "custom_model = WhisperForConditionalGeneration(custom_config)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring the WhisperConfig in detail\n",
    "from transformers import WhisperConfig\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "# Create a sample config and get all attributes\n",
    "config = WhisperConfig()\n",
    "\n",
    "# Print all configuration parameters with their default values\n",
    "print(\"=== WHISPER CONFIG PARAMETERS ===\")\n",
    "config_dict = config.to_dict()\n",
    "for key, value in config_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Show what methods are available on the config\n",
    "print(\"\\n=== CONFIG METHODS ===\")\n",
    "config_methods = [method for method in dir(config) if not method.startswith('_') and callable(getattr(config, method))]\n",
    "for method in config_methods:\n",
    "    print(f\"- {method}{inspect.signature(getattr(config, method))}\")\n",
    "\n",
    "# Explain the meaning of key config parameters\n",
    "print(\"\\n=== KEY CONFIG PARAMETERS EXPLAINED ===\")\n",
    "key_params = {\n",
    "    \"vocab_size\": \"Size of the tokenizer vocabulary\",\n",
    "    \"d_model\": \"Dimension of the model's hidden representations\",\n",
    "    \"encoder_layers\": \"Number of encoder layers in the Transformer\",\n",
    "    \"encoder_attention_heads\": \"Number of attention heads in encoder's multi-head attention\",\n",
    "    \"decoder_layers\": \"Number of decoder layers in the Transformer\",\n",
    "    \"decoder_attention_heads\": \"Number of attention heads in decoder's multi-head attention\",\n",
    "    \"max_source_positions\": \"Maximum sequence length for encoder input\",\n",
    "    \"max_target_positions\": \"Maximum sequence length for decoder output\",\n",
    "    \"activation_function\": \"Activation function used in the feed-forward networks\",\n",
    "    \"num_mel_bins\": \"Number of mel features for audio processing\",\n",
    "    \"dropout\": \"Dropout probability throughout the model\",\n",
    "    \"attention_dropout\": \"Dropout probability for attention weights\",\n",
    "    \"activation_dropout\": \"Dropout probability after activation in FFN\"\n",
    "}\n",
    "\n",
    "for param, explanation in key_params.items():\n",
    "    if param in config_dict:\n",
    "        print(f\"- {param}: {config_dict[param]}\")\n",
    "        print(f\"  {explanation}\")\n",
    "\n",
    "# Show the differences between model sizes\n",
    "print(\"\\n=== MODEL SIZE CONFIGURATIONS ===\")\n",
    "model_sizes = {\n",
    "    \"tiny\": {\"d_model\": 384, \"encoder_layers\": 4, \"decoder_layers\": 4},\n",
    "    \"base\": {\"d_model\": 512, \"encoder_layers\": 6, \"decoder_layers\": 6},\n",
    "    \"small\": {\"d_model\": 768, \"encoder_layers\": 12, \"decoder_layers\": 12},\n",
    "    \"medium\": {\"d_model\": 1024, \"encoder_layers\": 24, \"decoder_layers\": 24},\n",
    "    \"large\": {\"d_model\": 1280, \"encoder_layers\": 32, \"decoder_layers\": 32},\n",
    "}\n",
    "\n",
    "for size, params in model_sizes.items():\n",
    "    approx_params = (params[\"d_model\"] * params[\"d_model\"] * \n",
    "                    (params[\"encoder_layers\"] + params[\"decoder_layers\"]) * 4) / 1000000\n",
    "    print(f\"- {size.capitalize()} model:\")\n",
    "    print(f\"  d_model={params['d_model']}, encoder_layers={params['encoder_layers']}, \" \n",
    "          f\"decoder_layers={params['decoder_layers']}\")\n",
    "    print(f\"  Approximate parameters: ~{approx_params:.1f}M\")\n",
    "\n",
    "# Examine how config affects model initialization\n",
    "print(\"\\n=== HOW CONFIG INFLUENCES MODEL ARCHITECTURE ===\")\n",
    "print(\"The config parameters determine:\")\n",
    "print(\"1. Size of embedding matrices (vocab_size × d_model)\")\n",
    "print(\"2. Number of encoder and decoder layers\")\n",
    "print(\"3. Size of feed-forward networks (d_model × 4 × d_model)\")\n",
    "print(\"4. Number of attention heads in multi-head attention\")\n",
    "print(\"5. Audio preprocessing parameters (sampling_rate, num_mel_bins)\")\n",
    "\n",
    "# Show how to customize a config\n",
    "print(\"\\n=== CUSTOMIZING CONFIG EXAMPLE ===\")\n",
    "print(\"\"\"\n",
    "# Create a customized Whisper config\n",
    "custom_config = WhisperConfig(\n",
    "    vocab_size=51865,              # Keep original vocabulary\n",
    "    d_model=512,                   # Smaller dimension (like base model)\n",
    "    encoder_layers=8,              # Custom number of layers\n",
    "    decoder_layers=8,              # Custom number of layers\n",
    "    encoder_attention_heads=8,     # Number of attention heads\n",
    "    decoder_attention_heads=8,     # Number of attention heads\n",
    "    dropout=0.2,                   # Higher dropout for regularization\n",
    "    attention_dropout=0.1,         # Attention-specific dropout\n",
    "    activation_function=\"gelu\",    # Activation function\n",
    "    max_source_positions=1500,     # Max length for audio input\n",
    "    max_target_positions=448       # Max length for text output\n",
    ")\n",
    "\n",
    "# Initialize a model with custom config\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "custom_model = WhisperForConditionalGeneration(custom_config)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining Whisper components in detail\n",
    "\n",
    "# Load the base model for comparison\n",
    "base_model = AutoModel.from_pretrained(\"openai/whisper-medium\")\n",
    "\n",
    "# Examine the processor in detail\n",
    "print(\"=== PROCESSOR DETAILS ===\")\n",
    "print(f\"Components: {list(processor.__dict__.keys())}\")\n",
    "\n",
    "# Feature extractor examination\n",
    "print(\"\\nFeature Extractor Properties:\")\n",
    "feature_extractor_props = [attr for attr in dir(processor.feature_extractor) \n",
    "                          if not attr.startswith('_') and not callable(getattr(processor.feature_extractor, attr))]\n",
    "for prop in feature_extractor_props[:10]:  # Show first 10 properties\n",
    "    value = getattr(processor.feature_extractor, prop)\n",
    "    print(f\"- {prop}: {value}\")\n",
    "\n",
    "print(\"\\nHow audio is processed:\")\n",
    "print(f\"Sampling rate: {processor.feature_extractor.sampling_rate} Hz\")\n",
    "print(f\"Feature size: {processor.feature_extractor.feature_size}\")\n",
    "print(f\"Hop length: {processor.feature_extractor.hop_length}\")\n",
    "print(f\"Chunk length: {processor.feature_extractor.chunk_length} seconds\")\n",
    "\n",
    "# Tokenizer examination\n",
    "print(\"\\nTokenizer Details:\")\n",
    "print(f\"Vocabulary size: {processor.tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {processor.tokenizer.model_max_length}\")\n",
    "print(f\"Special tokens: {processor.tokenizer.all_special_tokens}\")\n",
    "\n",
    "# Sample of vocabulary\n",
    "print(\"\\nSample tokens from vocabulary:\")\n",
    "sample_tokens = list(processor.tokenizer.get_vocab().items())[:10]\n",
    "for token, id in sample_tokens:\n",
    "    print(f\"- '{token}': {id}\")\n",
    "\n",
    "# Compare model architectures\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "\n",
    "# Check for the language modeling head (key difference)\n",
    "print(\"\\nKey structural difference:\")\n",
    "for name, module in model.named_children():\n",
    "    if \"lm_head\" in name:\n",
    "        print(f\"- Found in WhisperForConditionalGeneration: {name} - {type(module)}\")\n",
    "        print(f\"  Shape: Input {module.in_features} → Output {module.out_features}\")\n",
    "\n",
    "# Show the generation methods\n",
    "print(\"\\nGeneration methods available:\")\n",
    "generation_methods = [method for method in dir(model) \n",
    "                     if \"generate\" in method and not method.startswith(\"_\")]\n",
    "for method in generation_methods:\n",
    "    print(f\"- {method}\")\n",
    "\n",
    "# Practical demonstration of prompt handling\n",
    "print(\"\\n=== PRACTICAL CAPABILITIES ===\")\n",
    "print(\"Special prompt capabilities for different languages and tasks:\")\n",
    "\n",
    "# Show available languages and tasks\n",
    "print(\"\\nAvailable languages in processor:\")\n",
    "if hasattr(processor, \"tokenizer\") and hasattr(processor.tokenizer, \"additional_special_tokens\"):\n",
    "    language_tokens = [token for token in processor.tokenizer.additional_special_tokens \n",
    "                      if token.startswith(\"<|\") and token.endswith(\"|>\") and len(token) < 10]\n",
    "    print(language_tokens[:10])  # Show first 10 language tokens\n",
    "\n",
    "print(\"\\nPrompt generation for different tasks:\")\n",
    "for task in [\"transcribe\", \"translate\"]:\n",
    "    for language in [\"english\", \"french\", \"japanese\"]:\n",
    "        prompt_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "        prompt_tokens = processor.tokenizer.convert_ids_to_tokens(prompt_ids)\n",
    "        print(f\"- {language.capitalize()} {task}: {prompt_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-audio-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
