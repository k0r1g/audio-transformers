Using device: cuda
Loading model and processor...
model.safetensors: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 151M/151M [00:01<00:00, 94.4MB/s]
Loading dataset...
README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4.07k/4.07k [00:00<00:00, 5.17MB/s]
train-00000-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 473M/473M [00:10<00:00, 46.2MB/s]
train-00001-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 582M/582M [00:12<00:00, 45.2MB/s]
train-00002-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 505M/505M [00:10<00:00, 46.4MB/s]
train-00003-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 466M/466M [00:10<00:00, 45.7MB/s]
train-00004-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 522M/522M [00:11<00:00, 45.3MB/s]
train-00005-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 473M/473M [00:10<00:00, 46.4MB/s]
train-00006-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 406M/406M [00:08<00:00, 45.9MB/s]
train-00007-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 494M/494M [00:10<00:00, 46.0MB/s]
train-00008-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 449M/449M [00:09<00:00, 45.8MB/s]
train-00009-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 428M/428M [00:09<00:00, 46.4MB/s]
train-00010-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 513M/513M [00:11<00:00, 46.3MB/s]
train-00011-of-00012.parquet: 100%|████████████████████████████████████████████████████████████████████████████████████| 450M/450M [00:09<00:00, 46.6MB/s]
Generating train split: 100%|███████████████████████████████████████████████████████████████████████████████| 11615/11615 [00:43<00:00, 269.05 examples/s]
Using 10.00% of the data: 1161 samples.
Loaded 928 samples from train split
Number of styles: 9
Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:33<00:00,  3.46 examples/s]
Loaded 116 samples from validation split
Number of styles: 8
Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:01<00:00, 64.36 examples/s]
Loaded 117 samples from test split
Number of styles: 9
Model: EmotionWhisperModel(
  (whisper): WhisperModel(
    (encoder): WhisperEncoder(
      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))
      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))
      (embed_positions): Embedding(1500, 384)
      (layers): ModuleList(
        (0-3): 4 x WhisperEncoderLayer(
          (self_attn): WhisperSdpaAttention(
            (k_proj): Linear(in_features=384, out_features=384, bias=False)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (out_proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (activation_fn): GELUActivation()
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): WhisperDecoder(
      (embed_tokens): Embedding(51865, 384, padding_idx=50257)
      (embed_positions): WhisperPositionalEmbedding(448, 384)
      (layers): ModuleList(
        (0-3): 4 x WhisperDecoderLayer(
          (self_attn): WhisperSdpaAttention(
            (k_proj): Linear(in_features=384, out_features=384, bias=False)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (out_proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (activation_fn): GELUActivation()
          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): WhisperSdpaAttention(
            (k_proj): Linear(in_features=384, out_features=384, bias=False)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (out_proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proj_out): Linear(in_features=384, out_features=51865, bias=False)
  (emotion_classifier): Linear(in_features=384, out_features=26, bias=True)
)
Starting training...
Epoch 1/10
Training:   0%|                                                                                                                   | 0/186 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/root/audio-transformers/whisper_finetune/train.py", line 368, in <module>
    train()
  File "/root/audio-transformers/whisper_finetune/train.py", line 169, in train
    for batch in progress_bar:
  File "/usr/local/lib/python3.10/dist-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/root/audio-transformers/whisper_finetune/dataset.py", line 48, in __getitem__
    with self.processor.as_target_processor(): #set it into the tokeniser mode
AttributeError: 'WhisperProcessor' object has no attribute 'as_target_processor'
Traceback (most recent call last):
  File "/root/audio-transformers/whisper_finetune/train.py", line 368, in <module>
    train()
  File "/root/audio-transformers/whisper_finetune/train.py", line 169, in train
    for batch in progress_bar:
  File "/usr/local/lib/python3.10/dist-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/root/audio-transformers/whisper_finetune/dataset.py", line 48, in __getitem__
    with self.processor.as_target_processor(): #set it into the tokeniser mode
AttributeError: 'WhisperProcessor' object has no attribute 'as_target_processor'
